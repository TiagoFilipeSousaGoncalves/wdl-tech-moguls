{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "6G0P8SnbpnaR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#PONTOS DE INTERESSE\n",
    "\n",
    "tmp = requests.get('https://wdl-data.fra1.digitaloceanspaces.com/porto-digital/POIs.json').json()\n",
    "ret = []\n",
    "\n",
    "for count, point_of_interest in enumerate(tmp['points_of_interest']):\n",
    "    category = point_of_interest.get('category')[0]['value']\n",
    "    location = point_of_interest.get('location')\n",
    "    created = point_of_interest.get('created')\n",
    "    \n",
    "    if 'point' in location:\n",
    "        location = location.get('point')[0].get('Point').get('posList')\n",
    "    else:\n",
    "        location = np.nan\n",
    "    description = point_of_interest.get('description')[0]['value']\n",
    "    \n",
    "    ret.append({\n",
    "        'id': count,\n",
    "        'created_at': created,\n",
    "        'category': category,\n",
    "        'location': location,\n",
    "        'description': description\n",
    "    })\n",
    "\n",
    "df_pois = pd.DataFrame.from_records(ret)\n",
    "\n",
    "df_pois[['latitude', 'longitude']] = df_pois['location'].str.split(' ', 1, expand=True)\n",
    "df_pois = df_pois.drop(1609) #NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ab0qaylvprzR"
   },
   "outputs": [],
   "source": [
    "#SENSORES\n",
    "\n",
    "data_entities = pd.read_csv('/content/drive/MyDrive/WDL/data_entities.csv', sep=',', encoding='latin-1', error_bad_lines=False)\n",
    "indexNames = data_entities[(data_entities['entity_id'] == 'testsixsq')].index\n",
    "data_entities.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "KnR2Oc8Mpr4c"
   },
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "def get_lat_lon_dist(row):\n",
    "    latlon1 = tuple(row[['latitude1', 'longitude1']])\n",
    "    latlon2 = tuple(row[['latitude2', 'longitude2']])\n",
    "\n",
    "    return geopy.distance.geodesic(latlon1, latlon2).kilometers\n",
    "\n",
    "\n",
    "# Cross-join to get all combinations of lat/lon.\n",
    "dist = pd.merge(data_entities.assign(k=1), df_pois.assign(k=1), on='k', suffixes=('1', '2')) \\\n",
    "         .drop('k', axis=1)\n",
    "\n",
    " \n",
    "dist['dist_NOME_DA_ENTIDADE'] = dist.apply(get_lat_lon_dist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "AaEojhF1v7B0"
   },
   "outputs": [],
   "source": [
    "dist_new=dist[['entity_id', 'id', 'category', 'dist_NOME_DA_ENTIDADE']].copy() \n",
    "dist_new = dist_new.rename(columns = {'id': 'point of interest', 'entity_id': 'sensor'}, inplace = False)\n",
    "dist_new['is_below_threshold'] = np.where(dist_new['dist_NOME_DA_ENTIDADE']<=1.5, 1, 0)\n",
    "sensor_categ=dist_new.groupby(['sensor', 'category'])['is_below_threshold'].sum().reset_index()\n",
    "sensor_categ=sensor_categ.pivot_table(index=\"sensor\", columns=\"category\", values=\"is_below_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sqTz7az611K",
    "outputId": "e69c1c98-cff5-4344-c747-7a534123aae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Reading file  traffic_flow_2018.csv\n",
      "Reading file  traffic_flow_2019.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (4,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "TRAFFIC_DATA_PATH = [\n",
    "    'traffic_flow_2018.csv',\n",
    "    'traffic_flow_2019.csv'\n",
    "]\n",
    "\n",
    "\n",
    "def load_data(data_paths, date_col, value_cols, agg_operation = 'sum'):\n",
    "    concat_data = []\n",
    "\n",
    "    for file in data_paths:\n",
    "        print(\"Reading file \", file)\n",
    "        df = pd.read_csv('/content/drive/MyDrive/WDL/' + file, encoding='latin-1')\n",
    "        \n",
    "        # print(df.head())\n",
    "\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df_resampled = df.set_index(date_col).groupby('entity_id')[value_cols].resample('H').agg(agg_operation).reset_index()\n",
    "        concat_data.append(df_resampled)\n",
    "\n",
    "    return pd.concat(concat_data)\n",
    "\n",
    "date_col = 'dateobservedfrom'\n",
    "value_cols = ['intensity']\n",
    "files_name = TRAFFIC_DATA_PATH\n",
    "traffic_data = load_data(files_name, date_col, value_cols, agg_operation = 'sum')\n",
    "traffic_data.rename(columns={'dateobservedfrom': 'dateobserved', 'entity_id':'sensor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "qpPzPn-spSh6"
   },
   "outputs": [],
   "source": [
    "traffic_categ = pd.merge(sensor_categ, traffic_data, on=\"sensor\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
