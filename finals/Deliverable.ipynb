{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeP6xkRIBJco"
   },
   "source": [
    "tmp# World Data League 2021\n",
    "## Notebook Template\n",
    "\n",
    "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
    "\n",
    "The notebook must:\n",
    "\n",
    "*   üíª have all the code that you want the jury to evaluate\n",
    "*   üß± follow the predefined structure\n",
    "*   üìÑ have markdown descriptions where you find necessary\n",
    "*   üëÄ be saved with all the output that you want the jury to see\n",
    "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6yLp6lm89xR"
   },
   "source": [
    "## Authors\n",
    "Write the name (first and last) of the people on your team that are responsible for developing this solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ2xq4469UnN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QNcZrkVu9xf"
   },
   "source": [
    "## External links and resources\n",
    "Paste here all the links to external resources that are necessary to understand and run your code. Add descriptions to make it clear how to use them during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJzSXXIYvxf9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63ltgxp_rOpI"
   },
   "source": [
    "## Introduction\n",
    "Describe how you framed the challenge by telling us what problem are you trying to solve and how your solution solves that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp34gOznrwrq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8rCpNajszur"
   },
   "source": [
    "## Development\n",
    "Start coding here! üë©‚Äçüíª\n",
    "\n",
    "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
    "\n",
    "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import pandas as pd\n",
    "\n",
    "def read_shapefile(shp_path):\n",
    "    \"\"\"\n",
    "    Read a shapefile into a Pandas dataframe with a 'coords' column holding\n",
    "    the geometry information. This uses the pyshp package\n",
    "    \"\"\"\n",
    "\n",
    "    #read file, parse out the records and shapes\n",
    "    sf = shapefile.Reader(shp_path)\n",
    "    fields = [x[0] for x in sf.fields][1:]\n",
    "    records = sf.records()\n",
    "    shps = [s.points for s in sf.shapes()]\n",
    "\n",
    "    #write into a dataframe\n",
    "    df = pd.DataFrame(columns=fields, data=records)\n",
    "    df = df.assign(coords=shps)\n",
    "\n",
    "    return df, sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, shps  = read_shapefile('data/noise_sensor_GIS/Rilievi openoise.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, shps  = read_shapefile('data/zipfolder/RUM_10_zon_acustica_Layer.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link: https://webgis.arpa.piemonte.it/Geoviewer2D/?config=other-configs/acustica_config.json\n",
    "\n",
    "mapping_location_area_code = pd.DataFrame(\n",
    "    [['s_01', 65, 55, 'IV - Aree di intensa attivit√† umana'],\n",
    "    ['s_02', 60, 50, 'III - Aree di tipo misto'],\n",
    "    ['s_03', 60, 50, 'III - Aree di tipo misto'],\n",
    "    ['s_05', 65, 55, 'IV - Aree di intensa attivit√† umana'],\n",
    "    ['s_06', 60, 50, 'III - Aree di tipo misto']],\n",
    "    columns=['code', 'day_max_db', 'night_max_db', 'area_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = sensor_list.merge(mapping_location_area_code, on=['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afB4W0KnutpV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = pd.read_csv('data/noise_sensor_list.csv', sep = ';')\n",
    "sensor_list['Sensor_ID'] = ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "\n",
    "sensor_list['Lat'] = sensor_list['Lat'].str.replace(',', '.').astype(float)\n",
    "sensor_list['Long'] = sensor_list['Long'].str.replace(',', '.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two other noise sensors in the following locations, outside of San Salvario,\n",
    "with an increasing busy nightlife: TTO-001, TT0-002 that are outside San Salvario. Ignored for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noise_data(file_paths, sensor_list):\n",
    "    \"\"\"\n",
    "    Function for loading noise data into the correct format\n",
    "    \"\"\"\n",
    "    concat_data = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=8, sep=';')\n",
    "        df = df.melt(id_vars=['Data', 'Ora'])\n",
    "        df['Timestamp'] = pd.to_datetime(df['Data'] + ' ' + df['Ora'])\n",
    "        df.columns = ['Date', 'Time', 'Sensor_ID', 'Intensity', 'Timestamp']\n",
    "        df['Intensity'] = df['Intensity'].str.replace(',', '.').astype(float)\n",
    "\n",
    "        concat_data.append(df)\n",
    "\n",
    "    concat_df = pd.concat(concat_data)\n",
    "\n",
    "    output = concat_df.merge(sensor_list, on=['Sensor_ID'])\n",
    "    return output[['Timestamp', 'Sensor_ID', 'Intensity', 'address', 'Lat', 'Long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_noise_data(file_paths, sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4jPpcKOutZ5"
   },
   "outputs": [],
   "source": [
    "file_paths = ['data/noise_data/san_salvario_2016.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Police complaints\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "file_paths=glob.glob('data/police_complaints/*.csv')\n",
    "\n",
    "concat_data = []\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file, sep=',')\n",
    "    df['Timestamp'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    concat_data.append(df)\n",
    "df_final = pd.concat(concat_data)\n",
    "\n",
    "filter = ['Facilities disturbances', 'Disturbing noises', 'Youth aggregation']\n",
    "df_filtered = df_final.loc[df_final['Criminal sub-category'].isin(filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criminal category</th>\n",
       "      <th>Criminal sub-category</th>\n",
       "      <th>District</th>\n",
       "      <th>Localization</th>\n",
       "      <th>Green Area</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Facilities disturbances</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NIZZA/(VIA)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/02/2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Facilities disturbances</td>\n",
       "      <td>8.0</td>\n",
       "      <td>VIRGILIO/(VIALE)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23/03/2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Facilities disturbances</td>\n",
       "      <td>8.0</td>\n",
       "      <td>BELFIORE/(VIA)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28/03/2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Facilities disturbances</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MONCALIERI/(CORSO)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21/04/2017</td>\n",
       "      <td>10.37</td>\n",
       "      <td>2017-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Facilities disturbances</td>\n",
       "      <td>8.0</td>\n",
       "      <td>BIDONE/GIORGIO (VIA)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26/04/2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Disturbing noises</td>\n",
       "      <td>8.0</td>\n",
       "      <td>GALILEO GALILEI/(CORSO)                       ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/08/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Disturbing noises</td>\n",
       "      <td>8.0</td>\n",
       "      <td>LANZA/GIOVANNI (CORSO)                        ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28/08/2019</td>\n",
       "      <td>12.54</td>\n",
       "      <td>2019-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Disturbing noises</td>\n",
       "      <td>8.0</td>\n",
       "      <td>GIACOSA/GIUSEPPE (VIA)                        ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30/08/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-08-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Disturbing noises</td>\n",
       "      <td>8.0</td>\n",
       "      <td>S. ANSELMO/(VIA)                              ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/09/2019</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2019-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Civil coexistence</td>\n",
       "      <td>Disturbing noises</td>\n",
       "      <td>8.0</td>\n",
       "      <td>BELFIORE/(VIA)                                ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25/11/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Criminal category    Criminal sub-category  District  \\\n",
       "519  Civil coexistence  Facilities disturbances       8.0   \n",
       "531  Civil coexistence  Facilities disturbances       8.0   \n",
       "534  Civil coexistence  Facilities disturbances       8.0   \n",
       "545  Civil coexistence  Facilities disturbances       8.0   \n",
       "548  Civil coexistence  Facilities disturbances       8.0   \n",
       "..                 ...                      ...       ...   \n",
       "390  Civil coexistence        Disturbing noises       8.0   \n",
       "398  Civil coexistence        Disturbing noises       8.0   \n",
       "401  Civil coexistence        Disturbing noises       8.0   \n",
       "413  Civil coexistence        Disturbing noises       8.0   \n",
       "433  Civil coexistence        Disturbing noises       8.0   \n",
       "\n",
       "                                          Localization Green Area        Date  \\\n",
       "519                                        NIZZA/(VIA)        NaN  11/02/2017   \n",
       "531                                   VIRGILIO/(VIALE)        NaN  23/03/2017   \n",
       "534                                     BELFIORE/(VIA)        NaN  28/03/2017   \n",
       "545                                 MONCALIERI/(CORSO)        NaN  21/04/2017   \n",
       "548                               BIDONE/GIORGIO (VIA)        NaN  26/04/2017   \n",
       "..                                                 ...        ...         ...   \n",
       "390  GALILEO GALILEI/(CORSO)                       ...        NaN  01/08/2019   \n",
       "398  LANZA/GIOVANNI (CORSO)                        ...        NaN  28/08/2019   \n",
       "401  GIACOSA/GIUSEPPE (VIA)                        ...        NaN  30/08/2019   \n",
       "413  S. ANSELMO/(VIA)                              ...        NaN  12/09/2019   \n",
       "433  BELFIORE/(VIA)                                ...        NaN  25/11/2019   \n",
       "\n",
       "      Hour  Timestamp  \n",
       "519    NaN 2017-02-11  \n",
       "531    NaN 2017-03-23  \n",
       "534    NaN 2017-03-28  \n",
       "545  10.37 2017-04-21  \n",
       "548    NaN 2017-04-26  \n",
       "..     ...        ...  \n",
       "390    NaN 2019-08-01  \n",
       "398  12.54 2019-08-28  \n",
       "401    NaN 2019-08-30  \n",
       "413   7.20 2019-09-12  \n",
       "433    NaN 2019-11-25  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[df_filtered.District == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Description here about the sensors' proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "m = folium.Map(location=[45.0530, 7.6798], zoom_start=15)\n",
    "\n",
    "for indice, row in sensor_list.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"Lat\"], row[\"Long\"]],\n",
    "        popup=row['address'],\n",
    "        icon=folium.Icon(color=\"red\", icon='automobile', prefix='fa')\n",
    "        ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sazonality and regular behavior studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "import numpy as np\n",
    "\n",
    "it_holidays = holidays.CountryHoliday('Italy')\n",
    "\n",
    "# We created a function to get some interesting date features, based on Pandas DataSeries predefined functions\n",
    "def get_date_features(df_resampled, date_col, suffix, holidays_list):\n",
    "    \"\"\"\n",
    "    Function for getting date features from a datetime column. \n",
    "    \"\"\"\n",
    "    df_resampled[f'day_{suffix}'] = df_resampled[date_col].dt.day\n",
    "    df_resampled[f'hour_{suffix}'] = df_resampled[date_col].dt.hour\n",
    "    df_resampled[f'month_{suffix}'] = df_resampled[date_col].dt.month\n",
    "    df_resampled[f'dayofweek_{suffix}'] = df_resampled[date_col].dt.dayofweek\n",
    "    # df_resampled[f'year_{suffix}'] = df_resampled[date_col].dt.year\n",
    "    df_resampled[f'quarter_{suffix}'] = df_resampled[date_col].dt.quarter\n",
    "    df_resampled[f'is_holiday_{suffix}'] = df_resampled[date_col].apply(lambda x: x in holidays_list)\n",
    "    # df_resampled[f'is_year_end_{suffix}'] = df_resampled[date_col].dt.is_year_end\n",
    "    df_resampled[f'is_weekend_{suffix}'] = np.where(df_resampled[f'dayofweek_{suffix}'].isin([5, 6]), 1, 0)\n",
    "                                                  \n",
    "    return df_resampled\n",
    "\n",
    "df = get_date_features(df, date_col='Timestamp', suffix='today', holidays_list=it_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbmean(levels, axis=None):\n",
    "    \"\"\"\n",
    "    Energetic average of levels.\n",
    "    :param levels: Sequence of levels.\n",
    "    :param axis: Axis over which to perform the operation.\n",
    "    .. math:: L_{mean} = 10 \\\\log_{10}{\\\\frac{1}{n}\\\\sum_{i=0}^n{10^{L/10}}}\n",
    "    \"\"\"\n",
    "    # levels = np.asanyarray(levels)\n",
    "    return 10.0 * np.log10((10.0**(levels / 10.0)).mean(axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def average_log(x):\n",
    "#    return 10 ** np.log(1 + np.mean(10**(x/10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_intensity_per_hour = df[df.Sensor_ID == 'C1'].groupby('hour_today')['Intensity'].apply(dbmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(avg_intensity_per_hour)), avg_intensity_per_hour.values)\n",
    "plt.title('Average sensor behavior for sensor C1 in the different times of day')\n",
    "plt.ylabel('Noise (dB)')\n",
    "plt.xlabel('Hour of day (h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ90XveiBN6A"
   },
   "outputs": [],
   "source": [
    "# We create a function to create our targets\n",
    "# As you can see, we created our target (label) based on a date offset (i.e., our label will be the intensity of the next day at the same time)\n",
    "def create_target(df_resampled, date_col = 'Timestamp', target_col = 'Intensity', entity_id='Sensor_ID', date_offset = 24):\n",
    "    \"\"\"\n",
    "    Function from creating lagged or future features for a specific date offset.\n",
    "    For instance, this adds a new column with the intensity values 24 hours in the future, for each row, by default.    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_resampled[f'date_col_{target_col}'] = df_resampled[date_col] + pd.DateOffset(hours=date_offset)\n",
    "    tmp = df_resampled[[entity_id, date_col, f'date_col_{target_col}', target_col]].merge(\n",
    "        df_resampled[[entity_id, date_col, f'date_col_{target_col}', target_col]], \n",
    "        left_on = [entity_id, f'date_col_{target_col}'], \n",
    "        right_on=[entity_id, date_col], \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    tmp = tmp[[entity_id, f'{date_col}_x', f'{target_col}_y']]\n",
    "    tmp.columns = [entity_id, date_col, f'target_{target_col}_{str(date_offset)}h']\n",
    "\n",
    "    df_resampled = df_resampled.merge(tmp, on=[entity_id, date_col])\n",
    "    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_target(data, target_col='Intensity', date_offset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSDath2nr1fq"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "### Scalability and Impact\n",
    "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGmbES9GszEv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XBiBOyAl2Sv"
   },
   "source": [
    "### Future Work\n",
    "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gK3heTKl7qz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook Submission Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
