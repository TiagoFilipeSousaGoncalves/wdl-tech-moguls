{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeP6xkRIBJco"
   },
   "source": [
    "tmp# World Data League 2021\n",
    "## Notebook Template\n",
    "\n",
    "This notebook is one of the mandatory deliverables when you submit your solution (alongside the video pitch). Its structure follows the WDL evaluation criteria and it has dedicated cells where you can add descriptions. Make sure your code is readable as it will be the only technical support the jury will have to evaluate your work.\n",
    "\n",
    "The notebook must:\n",
    "\n",
    "*   üíª have all the code that you want the jury to evaluate\n",
    "*   üß± follow the predefined structure\n",
    "*   üìÑ have markdown descriptions where you find necessary\n",
    "*   üëÄ be saved with all the output that you want the jury to see\n",
    "*   üèÉ‚Äç‚ôÇÔ∏è be runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6yLp6lm89xR"
   },
   "source": [
    "## Authors\n",
    "Write the name (first and last) of the people on your team that are responsible for developing this solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ2xq4469UnN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QNcZrkVu9xf"
   },
   "source": [
    "## External links and resources\n",
    "Paste here all the links to external resources that are necessary to understand and run your code. Add descriptions to make it clear how to use them during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJzSXXIYvxf9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63ltgxp_rOpI"
   },
   "source": [
    "## Introduction\n",
    "Describe how you framed the challenge by telling us what problem are you trying to solve and how your solution solves that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp34gOznrwrq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8rCpNajszur"
   },
   "source": [
    "## Development\n",
    "Start coding here! üë©‚Äçüíª\n",
    "\n",
    "Don't hesitate to create markdown cells to include descriptions of your work where you see fit, as well as commenting your code.\n",
    "\n",
    "We know that you know exactly where to start when it comes to crunching data and building models, but don't forget that WDL is all about social impact...so take that into consideration as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sstd_CeKAfc1",
    "outputId": "404b7857-cc26-4681-aa02-b1eff56a0576"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d457ef32915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshapefile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import shapefile\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzYPutu0Afc3"
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading data\n",
    "\n",
    "def read_shapefile(shp_path):\n",
    "    \"\"\"\n",
    "    Read a shapefile into a Pandas dataframe with a 'coords' column holding\n",
    "    the geometry information. This uses the pyshp package\n",
    "    \"\"\"\n",
    "\n",
    "    #read file, parse out the records and shapes\n",
    "    sf = shapefile.Reader(shp_path)\n",
    "    fields = [x[0] for x in sf.fields][1:]\n",
    "    records = sf.records()\n",
    "    shps = [s.points for s in sf.shapes()]\n",
    "\n",
    "    #write into a dataframe\n",
    "    df = pd.DataFrame(columns=fields, data=records)\n",
    "    df = df.assign(coords=shps)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function: Load Noise Data\n",
    "def load_noise_data(file_paths, sensor_list):\n",
    "    \"\"\"\n",
    "    Function for loading noise data into the correct format\n",
    "    \"\"\"\n",
    "    concat_data = []\n",
    "    for file in file_paths:\n",
    "        df = pd.read_csv(file, header=8, sep=';')\n",
    "        df = df.melt(id_vars=['Data', 'Ora'])\n",
    "        df['Timestamp'] = pd.to_datetime(df['Data'] + ' ' + df['Ora'])\n",
    "        df.columns = ['Date', 'Time', 'Sensor_ID', 'Intensity', 'Timestamp']\n",
    "        df['Intensity'] = df['Intensity'].str.replace(',', '.').astype(float)\n",
    "\n",
    "        concat_data.append(df)\n",
    "\n",
    "    concat_df = pd.concat(concat_data)\n",
    "\n",
    "    output = concat_df.merge(sensor_list, on=['Sensor_ID'])\n",
    "    \n",
    "    return output[['Timestamp', 'Sensor_ID', 'Intensity', 'address', 'Lat', 'Long', 'day_max_db', 'night_max_db', 'area_type']]\n",
    "\n",
    "\n",
    "\n",
    "# Function that calculates the distance in km between two points using the latitude and longitude data \n",
    "def get_lat_lon_dist(row):\n",
    "    \n",
    "    latlon1 = tuple(row[['latitude1', 'longitude1']])\n",
    "    latlon2 = tuple(row[['latitude2', 'longitude2']])\n",
    "\n",
    "    return geodesic(latlon1, latlon2).kilometers\n",
    "\n",
    "\n",
    "\n",
    "def compute_distances(df_pois, data_entities):\n",
    "\n",
    "    # In order to be able to apply the function defined above with the data from the file with the points of interest, \n",
    "    data_entities[['latitude', 'longitude']] = data_entities[[\"Lat\", \"Long\"]]\n",
    "    df_pois[['latitude', 'longitude']] = df_pois[[\"Latitude\", \"Longitude\"]]\n",
    "    \n",
    "\n",
    "    # Cross-join to get all combinations of latitude/longitude\n",
    "    dist = pd.merge(data_entities.copy().assign(k=1), df_pois.copy().assign(k=1), on='k', suffixes=('1', '2')).drop('k', axis=1)\n",
    "    \n",
    "    # Application of the get_lat_lon_dist function defined with the data from the points of interest and the sensors. \n",
    "    # Creation of a new column \"dist_NOME_DA_ENTIDADE\" with the distance in km from each sensor to each point of interest\n",
    "    dist['dist_NOME_DA_ENTIDADE'] = dist.apply(get_lat_lon_dist, axis=1)\n",
    "    \n",
    "    return dist\n",
    "\n",
    "\n",
    "def return_amount_of_points_of_interest_per_sensor(distances_df, threshold):\n",
    "    \n",
    "    # We set a thresold distance equal to 1.5km because we consider, that given the size of the Porto region, 1.5km is a walkable distance. \n",
    "    # For each sensor, we defined the number of restaurants, hotels, shopping centers, etc that are at a distance of 1.5km or less\n",
    "    dist_new = distances_df.copy()\n",
    "    dist_new['is_below_threshold'] = np.where(dist_new['dist_NOME_DA_ENTIDADE'] <= threshold, 1, 0)\n",
    "    \n",
    "    \n",
    "    # Check this later\n",
    "    # sensor_categ = dist_new.groupby(['entity_id', 'category'])['is_below_threshold'].sum().reset_index()\n",
    "    # sensor_categ = sensor_categ.pivot_table(index = \"entity_id\", columns = \"category\", values = \"is_below_threshold\")\n",
    "    \n",
    "    return dist_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmCuTK10Afc3"
   },
   "outputs": [],
   "source": [
    "# Load list of sensors\n",
    "\n",
    "sensor_list = pd.read_csv('data/noise_sensor_list.csv', sep = ';')\n",
    "sensor_list['Sensor_ID'] = ['C1', 'C2', 'C3', 'C4', 'C5']\n",
    "sensor_list['Lat'] = sensor_list['Lat'].str.replace(',', '.').astype(float)\n",
    "sensor_list['Long'] = sensor_list['Long'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Get mapping locations and correspondence to area type\n",
    "# Link: https://webgis.arpa.piemonte.it/Geoviewer2D/?config=other-configs/acustica_config.json\n",
    "\n",
    "mapping_location_area_code = pd.DataFrame(\n",
    "    [['s_01', 65, 55, 'IV - Aree di intensa attivit√† umana'],\n",
    "    ['s_02', 60, 50, 'III - Aree di tipo misto'],\n",
    "    ['s_03', 60, 50, 'III - Aree di tipo misto'],\n",
    "    ['s_05', 65, 55, 'IV - Aree di intensa attivit√† umana'],\n",
    "    ['s_06', 60, 50, 'III - Aree di tipo misto']],\n",
    "    columns=['code', 'day_max_db', 'night_max_db', 'area_type']\n",
    ")\n",
    "\n",
    "sensor_list = sensor_list.merge(mapping_location_area_code, on=['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vb9BwcPJAfc4",
    "outputId": "7a633925-a65b-4772-efb2-2862c1910846"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>address</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>streaming</th>\n",
       "      <th>Sensor_ID</th>\n",
       "      <th>day_max_db</th>\n",
       "      <th>night_max_db</th>\n",
       "      <th>area_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s_01</td>\n",
       "      <td>Via Saluzzo, 26 Torino</td>\n",
       "      <td>45.059172</td>\n",
       "      <td>7.678986</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "      <td>C1</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>IV - Aree di intensa attivit√† umana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s_02</td>\n",
       "      <td>Via Principe Tommaso, 18bis Torino</td>\n",
       "      <td>45.057837</td>\n",
       "      <td>7.681555</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "      <td>C2</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>III - Aree di tipo misto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s_03</td>\n",
       "      <td>Largo Saluzzo Torino</td>\n",
       "      <td>45.058518</td>\n",
       "      <td>7.678854</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "      <td>C3</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>III - Aree di tipo misto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s_05</td>\n",
       "      <td>Via Principe Tommaso angolo via Baretti Torino</td>\n",
       "      <td>45.057603</td>\n",
       "      <td>7.681348</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "      <td>C4</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>IV - Aree di intensa attivit√† umana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s_06</td>\n",
       "      <td>Corso Marconi, 27 Torino</td>\n",
       "      <td>45.055554</td>\n",
       "      <td>7.682590</td>\n",
       "      <td>https://userportal.smartdatanet.it/userportal/...</td>\n",
       "      <td>C5</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>III - Aree di tipo misto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                         address        Lat      Long  \\\n",
       "0  s_01                          Via Saluzzo, 26 Torino  45.059172  7.678986   \n",
       "1  s_02              Via Principe Tommaso, 18bis Torino  45.057837  7.681555   \n",
       "2  s_03                            Largo Saluzzo Torino  45.058518  7.678854   \n",
       "3  s_05  Via Principe Tommaso angolo via Baretti Torino  45.057603  7.681348   \n",
       "4  s_06                        Corso Marconi, 27 Torino  45.055554  7.682590   \n",
       "\n",
       "                                           streaming Sensor_ID  day_max_db  \\\n",
       "0  https://userportal.smartdatanet.it/userportal/...        C1          65   \n",
       "1  https://userportal.smartdatanet.it/userportal/...        C2          60   \n",
       "2  https://userportal.smartdatanet.it/userportal/...        C3          60   \n",
       "3  https://userportal.smartdatanet.it/userportal/...        C4          65   \n",
       "4  https://userportal.smartdatanet.it/userportal/...        C5          60   \n",
       "\n",
       "   night_max_db                            area_type  \n",
       "0            55  IV - Aree di intensa attivit√† umana  \n",
       "1            50             III - Aree di tipo misto  \n",
       "2            50             III - Aree di tipo misto  \n",
       "3            55  IV - Aree di intensa attivit√† umana  \n",
       "4            50             III - Aree di tipo misto  "
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvAIjP3zAfc4"
   },
   "outputs": [],
   "source": [
    "file_paths_noise_data = [\n",
    "    'data/noise_data/san_salvario_2016.csv',\n",
    "    'data/noise_data/san_salvario_2017.csv',\n",
    "    'data/noise_data/san_salvario_2018.csv',\n",
    "    'data/noise_data/san_salvario_2019.csv',\n",
    "]\n",
    "data = load_noise_data(file_paths_noise_data, sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XNMAVI_Afc5",
    "outputId": "ddf13ff0-990d-4d63-ea0a-f970c8aba0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-6197e77966b6>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered_san_salvario['Localization'] = df_filtered_san_salvario['Localization'].str.lower().str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Police complaints\n",
    "\n",
    "file_paths=glob.glob('data/police_complaints/*.csv')\n",
    "\n",
    "concat_data = []\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file, sep=',')\n",
    "    df['Timestamp'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    concat_data.append(df)\n",
    "df_final = pd.concat(concat_data)\n",
    "\n",
    "filter = ['Facilities disturbances', 'Disturbing noises', 'Youth aggregation']\n",
    "df_filtered = df_final.loc[df_final['Criminal sub-category'].isin(filter)]\n",
    "\n",
    "df_filtered_san_salvario = df_filtered[df_filtered.District == 8]\n",
    "\n",
    "df_filtered_san_salvario['Localization'] = df_filtered_san_salvario['Localization'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCOkGPL_Afc5"
   },
   "outputs": [],
   "source": [
    "localization_address_mapping = {\n",
    "   'principe tommaso/(via)':  'Via Principe Tommaso, 18bis Torino',\n",
    "   'baretti/giuseppe (via)': 'Via Principe Tommaso angolo via Baretti Torino',\n",
    "   'marconi/guglielmo (corso)' : 'Corso Marconi, 27 Torino',\n",
    "   'saluzzo/(largo)': 'Largo Saluzzo Torino',\n",
    "   'saluzzo/(via)': 'Via Saluzzo, 26 Torino'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aghsii1tAfc6",
    "outputId": "429adbc0-1194-4eb5-eeb4-8dd869cbba77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e10d5282f209>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered_san_salvario['address'] = df_filtered_san_salvario['Localization'].map(localization_address_mapping)\n"
     ]
    }
   ],
   "source": [
    "df_filtered_san_salvario['address'] = df_filtered_san_salvario['Localization'].map(localization_address_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbTc7zZGAfc7"
   },
   "outputs": [],
   "source": [
    "df_filtered_san_salvario = df_filtered_san_salvario[~df_filtered_san_salvario.address.isna()]\n",
    "                                                    \n",
    "# Tiago starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5vvVwIMAfc7"
   },
   "outputs": [],
   "source": [
    "def create_target_complaints(df_filtered_san_salvario, range_days = 2, target_forecast = 1):\n",
    "\n",
    "    complaint_dates = pd.to_datetime(df_filtered_san_salvario['Date']).unique()\n",
    "\n",
    "    # Target dates: same day or subtract one day (by default)\n",
    "    range_deltas = [pd.Timedelta(x + target_forecast, 'd') for x in np.arange(0, range_days)]\n",
    "    \n",
    "    target_dates = []\n",
    "    for x in range_deltas:\n",
    "        for y in complaint_dates:\n",
    "            target_dates.append(y - x)\n",
    "            \n",
    "    return target_dates, complaint_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14VDtxaWAfc7"
   },
   "outputs": [],
   "source": [
    "target_dates, complaint_dates = create_target_complaints(df_filtered_san_salvario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dULDy0XvAfc8"
   },
   "outputs": [],
   "source": [
    "# Queixa no dia 24\n",
    "# Houve barulho no 24 ou 23 \n",
    "# Queremos prever no 23 ou 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbI1j11BAfc8",
    "outputId": "88868a63-aa6b-4be4-e99f-5e711793d1b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Timestamp('2016-02-22 00:00:00'), Timestamp('2016-02-23 00:00:00')],\n",
       " numpy.datetime64('2016-02-24T00:00:00.000000000'))"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(target_dates)[0:2], sorted(complaint_dates)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ29io_MAfc8"
   },
   "outputs": [],
   "source": [
    "data['Timestamp_trunc'] = data['Timestamp'].truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9coJ02DsAfc9"
   },
   "outputs": [],
   "source": [
    "data['complaint_followed'] = np.where(data['Timestamp_trunc'].isin(target_dates), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uUFvFZBAfc9"
   },
   "outputs": [],
   "source": [
    "tmp = df_filtered_san_salvario.merge(data, on=['address'])\n",
    "\n",
    "# Filtrar as linhas em que timestamp_x >= timestamp_y + 1 dia \n",
    "\n",
    "# Criar um pandas Series / dicionario que tenha como chave o dia/timestamp no qual houve uma queixa no mesmo dia ou no dia seguinte (que vai ser a label=1)\n",
    "# label = 0 v√£o ser os dias em que n√£o se conseguiu mapear nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_b2V7VIAfc9"
   },
   "outputs": [],
   "source": [
    "# Filtrar dataset de treino para as datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYSSBNJFAfc-"
   },
   "outputs": [],
   "source": [
    "# Get the Points of Interest of this Region\n",
    "# We start by loading the .CSV file\n",
    "businesses = pd.read_csv('data/businesses.csv', delimiter=';')\n",
    "\n",
    "# Let's show \n",
    "businesses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LhdZ_KHAfc-"
   },
   "outputs": [],
   "source": [
    "# TODO (if we have time): Change this loop to a more efficient loop\n",
    "\n",
    "# To obtain the coordinates we performed a reversed mapping of the address\n",
    "# Create a Geolocator\n",
    "geolocator = Nominatim(user_agent=\"wdl-tech-moguls\")\n",
    "\n",
    "# Iterate through the businesses dataframe\n",
    "for i in range(len(businesses)):\n",
    "    location = geolocator.geocode(businesses.loc[i, \"ADDRESS\"])\n",
    "    businesses.loc[i, \"Longitude\"], businesses.loc[i, \"Latitude\"] = location.longitude, location.latitude\n",
    "\n",
    "    \n",
    "# We saved this into a .CSV for further use\n",
    "businesses.to_csv(\"data/businesses_proc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsnTwSSUAfc-"
   },
   "outputs": [],
   "source": [
    "# Let's check if it is everything OK\n",
    "businesses = pd.read_csv(\"data/businesses_proc.csv\")\n",
    "businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaX5TK_PAfc_"
   },
   "outputs": [],
   "source": [
    "# Let's now compute the distances between each point of interest and each sensor\n",
    "distances = compute_distances(businesses, sensor_list)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkWsDBB4Afc_"
   },
   "outputs": [],
   "source": [
    "# We save it into CSV for further use\n",
    "distances.to_csv(\"data/distances_sensors_pois.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etc9DJWPAfc_"
   },
   "outputs": [],
   "source": [
    "# Get different dataframes per threshold\n",
    "for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5]:\n",
    "    thresh_df = return_amount_of_points_of_interest_per_sensor(distances, thresh)\n",
    "    thresh_w_mapping = thresh_df.merge(mapping_location_area_code, on=['code'])\n",
    "    thresh_w_mapping.to_csv(f\"data/thresh_df_{thresh}.csv\")\n",
    "    \n",
    "# Shown an example\n",
    "thresh_w_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVFFTU_2AfdA"
   },
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    'code', \n",
    "    'address', \n",
    "    'Sensor_ID', \n",
    "    'day_max_db_x', \n",
    "    'night_max_db_x', \n",
    "    'area_type_x', \n",
    "    'TYPE', \n",
    "    'Description', \n",
    "    'Merchandise Type', \n",
    "    'dist_NOME_DA_ENTIDADE', \n",
    "    'is_below_threshold'\n",
    "]\n",
    "\n",
    "thresh_w_mapping = thresh_w_mapping[cols_to_keep]\n",
    "thresh_w_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72DOBF5jAfdB"
   },
   "outputs": [],
   "source": [
    "# Group by number of counts per category\n",
    "# Read the file we want (each file has a different threshold)\n",
    "# thresh_w_mapping = pd.read_csv(\"data/thresh_df_0.1.csv\")\n",
    "\n",
    "# We have some merchadises types that are not informative (they are given by numbers)\n",
    "cols_to_remove = ['205', '207', '208', '214', '217', '99']\n",
    "\n",
    "sensor_categ = thresh_w_mapping.copy().groupby(['Sensor_ID', 'Merchandise Type'])['is_below_threshold'].sum().reset_index()\n",
    "sensor_categ = sensor_categ.copy().pivot_table(index = \"Sensor_ID\", columns = \"Merchandise Type\", values = \"is_below_threshold\")\n",
    "sensor_categ = sensor_categ.copy().drop(cols_to_remove, axis = 1).reset_index()\n",
    "sensor_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aj1Nz_MbAfdC"
   },
   "outputs": [],
   "source": [
    "# Merge again to have the remaining columns (\"day_max_db\", \"night_max_db\", \"area_type\")\n",
    "final_df = sensor_categ.merge(sensor_list.copy()[[\"Sensor_ID\", \"day_max_db\", \"night_max_db\", \"area_type\"]], on=[\"Sensor_ID\"])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWNv9Cn4AfdC"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSUQ9qSNAfdD"
   },
   "source": [
    "## Sensor Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNOFwluqAfdE"
   },
   "source": [
    "TODO: Description here about the sensors' proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMNqNcrKAfdE"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "m = folium.Map(location=[45.0530, 7.6798], zoom_start=15)\n",
    "\n",
    "for indice, row in sensor_list.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row[\"Lat\"], row[\"Long\"]],\n",
    "        popup=row['address'],\n",
    "        icon=folium.Icon(color=\"red\", icon='automobile', prefix='fa')\n",
    "        ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVdd9UI2AfdF"
   },
   "source": [
    "## Sazonality and regular behavior studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8C0pF09AfdF"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "import numpy as np\n",
    "\n",
    "it_holidays = holidays.CountryHoliday('Italy')\n",
    "\n",
    "# We created a function to get some interesting date features, based on Pandas DataSeries predefined functions\n",
    "def get_date_features(df_resampled, date_col, suffix, holidays_list):\n",
    "    \"\"\"\n",
    "    Function for getting date features from a datetime column. \n",
    "    \"\"\"\n",
    "    df_resampled[f'day_{suffix}'] = df_resampled[date_col].dt.day\n",
    "    df_resampled[f'hour_{suffix}'] = df_resampled[date_col].dt.hour\n",
    "    df_resampled[f'month_{suffix}'] = df_resampled[date_col].dt.month\n",
    "    df_resampled[f'dayofweek_{suffix}'] = df_resampled[date_col].dt.dayofweek\n",
    "    # df_resampled[f'year_{suffix}'] = df_resampled[date_col].dt.year\n",
    "    df_resampled[f'quarter_{suffix}'] = df_resampled[date_col].dt.quarter\n",
    "    df_resampled[f'is_holiday_{suffix}'] = df_resampled[date_col].apply(lambda x: x in holidays_list)\n",
    "    # df_resampled[f'is_year_end_{suffix}'] = df_resampled[date_col].dt.is_year_end\n",
    "    df_resampled[f'is_weekend_{suffix}'] = np.where(df_resampled[f'dayofweek_{suffix}'].isin([5, 6]), 1, 0)\n",
    "                                                  \n",
    "    return df_resampled\n",
    "\n",
    "data = get_date_features(data, date_col='Timestamp', suffix='now', holidays_list=it_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA_45uFmAfdG"
   },
   "outputs": [],
   "source": [
    "def noise_threshold(data, date_col='hour_now', suffix='now', value_col='Intensity'):\n",
    "    mask_day = (data[date_col] > 6) & (data[date_col] < 22) & (data[value_col] > data['day_max_db'])\n",
    "    mask_night = (data[date_col] > 22) | (data[date_col] < 6) & (data[value_col] > data['night_max_db'])\n",
    "    mask = mask_day | mask_night\n",
    "\n",
    "    data[f'noise_exceeds_threshold_{suffix}'] = np.where(mask, 1, 0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wo9yo6IAfdG"
   },
   "outputs": [],
   "source": [
    "data = noise_threshold(data, date_col='hour_now', suffix='now', value_col='Intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCuSXWrhAi-5"
   },
   "outputs": [],
   "source": [
    "def current_db(data, date_col='hour_now'):\n",
    "    mask_day = (data[date_col] > 6) & (data[date_col] < 22) \n",
    "    mask_night = (data[date_col] > 22) | (data[date_col] < 6) \n",
    "    mask = mask_day | mask_night\n",
    "\n",
    "    data[f'current_max_db_value'] = np.where(mask==mask_day, data[f'day_max_db'], data[f'night_max_db'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaoIf2a_AlK6"
   },
   "outputs": [],
   "source": [
    "data = current_db(data, date_col='hour_now')\n",
    "data['relative_diff'] = (data.Intensity - data.current_max_db_value\t) / data.Intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbbE4nabAfdH"
   },
   "outputs": [],
   "source": [
    "def dbmean(levels, axis=None):\n",
    "    \"\"\"\n",
    "    Energetic average of levels.\n",
    "    :param levels: Sequence of levels.\n",
    "    :param axis: Axis over which to perform the operation.\n",
    "    .. math:: L_{mean} = 10 \\\\log_{10}{\\\\frac{1}{n}\\\\sum_{i=0}^n{10^{L/10}}}\n",
    "    \"\"\"\n",
    "    # levels = np.asanyarray(levels)\n",
    "    return 10.0 * np.log10((10.0**(levels / 10.0)).mean(axis=axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vZ1sodhAfdH"
   },
   "outputs": [],
   "source": [
    "avg_intensity_per_hour = data[data.Sensor_ID == 'C1'].groupby('hour_now')['Intensity'].apply(dbmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4NErv28AfdH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(avg_intensity_per_hour)), avg_intensity_per_hour.values)\n",
    "plt.title('Average sensor behavior for sensor C1 in the different times of day')\n",
    "plt.ylabel('Noise (dB)')\n",
    "plt.xlabel('Hour of day (h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ90XveiBN6A"
   },
   "outputs": [],
   "source": [
    "# We create a function to create our targets\n",
    "# As you can see, we created our target (label) based on a date offset (i.e., our label will be the intensity of the next day at the same time)\n",
    "def create_target(df_resampled, date_col = 'Timestamp', target_col = 'Intensity', entity_id='Sensor_ID', date_offset = 24):\n",
    "    \"\"\"\n",
    "    Function from creating lagged or future features for a specific date offset.\n",
    "    For instance, this adds a new column with the intensity values 24 hours in the future, for each row, by default.    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_resampled[f'date_col_{target_col}'] = df_resampled[date_col] + pd.DateOffset(hours=date_offset)\n",
    "    tmp = df_resampled[[entity_id, date_col, f'date_col_{target_col}', target_col]].merge(\n",
    "        df_resampled[[entity_id, date_col, f'date_col_{target_col}', target_col]], \n",
    "        left_on = [entity_id, f'date_col_{target_col}'], \n",
    "        right_on=[entity_id, date_col], \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    tmp = tmp[[entity_id, f'{date_col}_x', f'{target_col}_y']]\n",
    "    tmp.columns = [entity_id, date_col, f'target_{target_col}_{str(date_offset)}h']\n",
    "\n",
    "    df_resampled = df_resampled.merge(tmp, on=[entity_id, date_col])\n",
    "    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AKGRmrmAfdI"
   },
   "outputs": [],
   "source": [
    "data = create_target(data, target_col='Intensity', date_offset=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3ooAaS8AfdI"
   },
   "outputs": [],
   "source": [
    "data = get_date_features(data, date_col='date_col_Intensity', suffix='target', holidays_list=it_holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7RyEcVwAfdJ"
   },
   "outputs": [],
   "source": [
    "data = noise_threshold(data, date_col='hour_target', suffix='target', value_col='target_Intensity_24h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgyVMYeaAfdJ"
   },
   "outputs": [],
   "source": [
    "# Avg noise intensity next 3 hours\n",
    "\n",
    "#indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=3)\n",
    "#data['average_intensity_next_3h'] = data.groupby(['Sensor_ID'])['Intensity'].rolling(window=indexer, min_periods=1).agg(dbmean).reset_index()['Intensity']\n",
    "# data = noise_threshold(data, date_col='hour_target', suffix='target_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTOJBikxAfdJ"
   },
   "source": [
    "## Train first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5A9C2U5AfdJ"
   },
   "outputs": [],
   "source": [
    "# We create a list of columns that we do not need to train our model\n",
    "COLS_TO_REMOVE = [\n",
    "    'Timestamp',\n",
    "    'Sensor_ID',\n",
    "    'address',\n",
    "    'Lat',\n",
    "    'Long',\n",
    "    'area_type',\n",
    "    'target_Intensity_24h',\n",
    "    'date_col_Intensity',\n",
    "    'noise_exceeds_threshold_target',\n",
    "    #'average_intensity_next_3h',\n",
    "    #'noise_exceeds_threshold_target_2'\n",
    "]\n",
    "\n",
    "# Based on the previous list, we create a new list with the features that we actually need!\n",
    "COLS_TO_KEEP = [x for x in data.columns if x not in COLS_TO_REMOVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8ayaww5AfdJ"
   },
   "outputs": [],
   "source": [
    "COLS_TO_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzF7hMGgAfdJ"
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by= ['Timestamp', 'Sensor_ID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c37iv2EiAfdK"
   },
   "outputs": [],
   "source": [
    "target_1 = 'target_Intensity_24h'\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train model\n",
    "# Please note that we use 80% of the data set as our train set!\n",
    "X_train = data[0:int(0.7*len(data))]\n",
    "\n",
    "# We remove the NaNs (labels that are NaNs)\n",
    "X_train = X_train[~X_train[target_1].isna()]\n",
    "\n",
    "# We use the remaining 20% as test set\n",
    "X_test = data[int(0.7*len(data)):]\n",
    "\n",
    "# We remove the NaNs (labels that are NaNs)\n",
    "X_test = X_test[~X_test[target_1].isna()]\n",
    "\n",
    "# Our labels column\n",
    "y_train = X_train['noise_exceeds_threshold_target']\n",
    "\n",
    "# We train an XGBoost Regressor. \n",
    "# Since it is a decision tree, it becomes easier to explain the decisions of our model\n",
    "xgb = XGBClassifier(n_estimators=100)\n",
    "\n",
    "# We train our model\n",
    "xgb.fit(X_train[COLS_TO_KEEP].fillna(9999).astype(float), y_train)\n",
    "\n",
    "y_pred = xgb.predict_proba(X_test[COLS_TO_KEEP].fillna(9999).astype(float))\n",
    "X_test['pred_score'] = y_pred[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn5iXqbCAfdK"
   },
   "outputs": [],
   "source": [
    "X_train.Timestamp.max(), X_test.Timestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6srbg0oQAfdK"
   },
   "outputs": [],
   "source": [
    "X_test[(X_test.noise_exceeds_threshold_target == 1)][['Timestamp', 'Sensor_ID', 'Intensity', 'target_Intensity_24h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHweqduTAfdK"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"ROC AUC\", roc_auc_score(X_test['noise_exceeds_threshold_target'], y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-ASWPJwAfdK"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "# Shap explanation\n",
    "\n",
    "# We now explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(xgb)\n",
    "shap_values = explainer(X_test[COLS_TO_KEEP].fillna(9999).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGYrqpmjAfdK"
   },
   "outputs": [],
   "source": [
    "# Let's get a nice plot with the shap values so you can have an intuition on the rationale behind the model learned by the XGBoost Regressor\n",
    "shap.plots.beeswarm(shap_values, max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSDath2nr1fq"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "### Scalability and Impact\n",
    "Tell us how applicable and scalable your solution is if you were to implement it in a city. Identify possible limitations and measure the potential social impact of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGmbES9GszEv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XBiBOyAl2Sv"
   },
   "source": [
    "### Future Work\n",
    "Now picture the following scenario: imagine you could have access to any type of data that could help you solve this challenge even better. What would that data be and how would it improve your solution? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gK3heTKl7qz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deliverable.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
